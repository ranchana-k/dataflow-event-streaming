steps:
  # 1. Install Dependencies (if needed)
  - name: 'python:3.10-slim' # Or a custom image with dependencies
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        pip install -r requirements.txt

  # 2. Run the Dataflow Job using gcloud
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    args:
      - 'dataflow'
      - 'jobs'
      - 'run'
      - 'streaming-purchase-alerts' # Job name
      - '--gcs-location=gs://$_BUCKET/main.py' # Path to your pipeline script
      - '--region=$_REGION'
      - '--temp-location=gs://$_BUCKET/temp'
      - '--staging-location=gs://$_BUCKET/staging'
      - '--parameters=config_file=pipeline_config.json'  # Pass config file path
      - '--project=$_PROJECT_ID'
      - '--runner=DataflowRunner'
      - '--service-account=dataflow-pipeline-sa@$_PROJECT_ID.iam.gserviceaccount.com' # Use a custom service account
      # Add other Dataflow parameters as needed
    entrypoint: 'gcloud'

# Substitutions (for variables)
substitutions:
  _REGION: 'us-central1'
  _AGG_WINDOW: "10"
  _ALERT_WINDOW: "1"
  _REGION: "us-central1"
  _BUCKET: "dataflow-test-ranchana"
  _PROJECT_ID: "rugged-precept-451103-n9"
  _PROJECT_NUMBER: "1046723826220"


timeout: 1200s

options:
  logging: CLOUD_LOGGING_ONLY



